diff --git a/models/language_modeling/tensorflow/bert_large/training/bfloat16/modeling.py b/models/language_modeling/tensorflow/bert_large/training/bfloat16/modeling.py
index f99b95d23..b59d4bd2e 100644
--- a/models/language_modeling/tensorflow/bert_large/training/bfloat16/modeling.py
+++ b/models/language_modeling/tensorflow/bert_large/training/bfloat16/modeling.py
@@ -27,7 +27,8 @@ import numpy as np
 import six
 import tensorflow as tf
 import generic_ops as bf
-
+from intel_extension_for_tensorflow.python.ops.multi_head_attention import scaled_dot_product_attention
+from intel_extension_for_tensorflow.python.ops.mlp import FusedDenseBiasAddGelu
 
 class BertConfig(object):
   """Configuration for `BertModel`."""
@@ -774,13 +775,6 @@ def attention_layer(from_tensor,
   key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,
                                    to_seq_length, size_per_head)
 
-  # Take the dot product between "query" and "key" to get the raw
-  # attention scores.
-  # `attention_scores` = [B, N, F, T]
-  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)
-  attention_scores = tf.multiply(attention_scores, bf.r_cast(
-                                 1.0 / math.sqrt(float(size_per_head))))
-
   if attention_mask is not None:
     # `attention_mask` = [B, 1, F, T]
     attention_mask = tf.expand_dims(attention_mask, axis=[1])
@@ -790,17 +784,6 @@ def attention_layer(from_tensor,
     # positions we want to attend and -10000.0 for masked positions.
     adder = (1.0 - tf.cast(attention_mask, from_tensor.dtype)) * -10000.0
 
-    # Since we are adding it to the raw scores before the softmax, this is
-    # effectively the same as removing these entirely.
-    attention_scores += adder
-
-  # Normalize the attention scores to probabilities.
-  # `attention_probs` = [B, N, F, T]
-  attention_probs = bf.softmax(attention_scores)
-
-  # This is actually dropping out entire tokens to attend to, which might
-  # seem a bit unusual, but is taken from the original Transformer paper.
-  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)
 
   # `value_layer` = [B, T, N, H]
   value_layer = tf.reshape(
@@ -810,11 +793,8 @@ def attention_layer(from_tensor,
   # `value_layer` = [B, N, T, H]
   value_layer = tf.transpose(a=value_layer, perm=[0, 2, 1, 3])
 
-  # `context_layer` = [B, N, F, H]
-  context_layer = tf.matmul(attention_probs, value_layer)
+  context_layer = scaled_dot_product_attention(query_layer, key_layer, value_layer, adder, attention_probs_dropout_prob, use_fast_attention=True)
 
-  # `context_layer` = [B, F, N, H]
-  context_layer = tf.transpose(a=context_layer, perm=[0, 2, 1, 3])
 
   if do_return_2d_tensor:
     # `context_layer` = [B*F, N*H]
@@ -942,12 +922,18 @@ def transformer_model(input_tensor,
           attention_output = layer_norm(attention_output + layer_input)
 
       # The activation is only applied to the "intermediate" hidden layer.
+      # with tf.compat.v1.variable_scope("intermediate"):
+      #   intermediate_output = tf.compat.v1.layers.dense(
+      #       attention_output,
+      #       intermediate_size,
+      #       activation=intermediate_act_fn,
+      #       kernel_initializer=create_initializer(initializer_range))
+
       with tf.compat.v1.variable_scope("intermediate"):
-        intermediate_output = tf.compat.v1.layers.dense(
-            attention_output,
-            intermediate_size,
-            activation=intermediate_act_fn,
-            kernel_initializer=create_initializer(initializer_range))
+        dense_layer = FusedDenseBiasAddGelu(intermediate_size,
+                         kernel_initializer=create_initializer(initializer_range))
+        intermediate_output = dense_layer(attention_output)
+        intermediate_output = tf.reshape(intermediate_output, [-1, intermediate_size])
 
       # Down-project back to `hidden_size` then add the residual.
       with tf.compat.v1.variable_scope("output"):

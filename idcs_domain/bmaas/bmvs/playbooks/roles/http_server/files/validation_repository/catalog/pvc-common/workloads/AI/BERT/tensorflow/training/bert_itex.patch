diff --git a/models/language_modeling/tensorflow/bert_large/training/bfloat16/generic_ops.py b/models/language_modeling/tensorflow/bert_large/training/bfloat16/generic_ops.py
index 70f9a0ea..db4f63f1 100755
--- a/models/language_modeling/tensorflow/bert_large/training/bfloat16/generic_ops.py
+++ b/models/language_modeling/tensorflow/bert_large/training/bfloat16/generic_ops.py
@@ -20,11 +20,17 @@ from __future__ import print_function
 
 import numpy as np
 import tensorflow as tf
+import intel_extension_for_tensorflow as itex
+
+try:
+    from tensorflow.keras.mixed_precsion import experimental as mixed_precision
+except:
+    from tensorflow.keras import  mixed_precision
 
 
 _inprecision = tf.float32
 _rprecision = tf.float32
-_keras_policy = tf.keras.mixed_precision.Policy("float32")
+_keras_policy = mixed_precision.Policy("float32")
 _use_optimized_softmax = False
 _use_experimental_gelu = False
 
@@ -32,7 +38,7 @@ def set_global_precision(dt):
   # Set Keras API precision
   global _keras_policy
   if dt == tf.bfloat16:
-    _keras_policy=tf.keras.mixed_precision.Policy("mixed_bfloat16")
+    _keras_policy=mixed_precision.Policy("mixed_bfloat16")
 
   # Set basic API precision
   set_rprecision(dt)
@@ -81,7 +87,7 @@ def softmax(scores, axis=None):
       return r_cast(rval)
 
 def layer_norm(inputs, begin_norm_axis, begin_params_axis, scope):
-    lnorm = tf.keras.layers.LayerNormalization(dtype=get_keras_policy())
+    lnorm = itex.ops.LayerNormalization(dtype=_rprecision)
     return lnorm(inputs)
 
 "Moved from modeling.py"
@@ -97,7 +103,7 @@ def gelu(x):
     `x` with the GELU activation applied.
   """
   if _use_experimental_gelu:
-    return tf.nn.gelu(features=x, approximate=True)
+    return itex.ops.gelu(features=x, approximate=True)
   else:
     x = i_cast(x)
     cdf = 0.5 * (1.0 + tf.tanh(

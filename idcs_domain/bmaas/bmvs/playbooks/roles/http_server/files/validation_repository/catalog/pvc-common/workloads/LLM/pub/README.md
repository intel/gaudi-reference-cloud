# Intel® Extension for PyTorch*(IPEX) for Large Language Model (LLM) Instructions
The [IPEX release v2.1.10](https://github.com/intel/intel-extension-for-pytorch/tree/v2.1.10%2Bxpu/examples/gpu/inference/python/llm) provides a lot of specific optimizations for LLM workloads on Intel® Data Center GPU Max Series. Please follow the IPEX online document and instructions to get start with LLM solutions on Intel Data Center GPU.   

## Intel Extension for Pytorch online Reference
- Public repo: https://github.com/intel/intel-extension-for-pytorch/tree/release/xpu/2.1.10
- Public documentation: https://intel.github.io/intel-extension-for-pytorch/xpu/2.1.10+xpu/
- Release note: https://github.com/intel/intel-extension-for-pytorch/releases/tag/v2.1.10+xpu 
- LLM on Intel GPU BKC: https://github.com/intel/intel-extension-for-pytorch/tree/v2.1.10%2Bxpu/examples/gpu/inference/python/llm 
- Large Language Model optimizations overview: https://intel.github.io/intel-extension-for-pytorch/xpu/2.1.10+xpu/tutorials/llm.html
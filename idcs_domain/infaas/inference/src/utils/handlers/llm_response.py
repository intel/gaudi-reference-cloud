import src.inference_engine.grpc_files.infaas_generate_pb2 as pb2
from src.utils.wrappers.backends.tgi import StreamResponse, ChatCompletionChunk
from src.utils.wrappers.transformers import HfTokenizer

import logging
from google.protobuf.json_format import MessageToDict
from text_generation.types import FinishReason
from typing import Dict, List, Any


class LlmResponseHandler:

    def __init__(
        self,
        tokenizer: HfTokenizer,
        inst_tokens: List[str]
    ) -> None:

        self._inst_tokens = inst_tokens
        self._tokenizer = tokenizer
        self._response = None

    async def handle_tgi_chat_completion_stream(
        self,
        response: ChatCompletionChunk,
        num_generated_tokens: int,
        **kwargs
    ) -> pb2.ChatCompletionStreamResponse:
        """cast the TGI chat completion returned object into the
           TextGenerator's ChatCompletionStream proto.

        Args:
            response (ChatCompletionChunk): TGI response object.
            num_generated_tokens (int): number of new tokens generated by
                                        the LLM.

        KwArgs:
            messages (List[Dict[str, str]]): input messages list, for prompt
                                             tokens handling.
            requestID (str): the internal MaaS requstID for logging.

        Returns:
            ChatCompletionStreamResponse: casting result.
        """
        request_id = kwargs.get("requestID")
        messages = kwargs.get("messages")

        choices = [
            pb2.ChatCompletionStreamChoice(
                index=choice.index,
                delta=pb2.ChatCompletionMessage(
                    role=choice.delta.role,
                    content=choice.delta.content
                ),
                logprobs=await self._cast_logprobs_data_safe(
                    request_id=request_id,
                    logprobs=choice.logprobs
                ),
                finish_reason=choice.finish_reason,
                usage=(
                    await self._get_usage(
                        messages, num_generated_tokens
                    ) if await self._is_eos_token(choice.delta.content)
                    else None
                )
            ) for choice in response.choices
        ]

        return pb2.ChatCompletionStreamResponse(
            completionID=response.id,
            object=response.object,
            created=response.created,
            model=response.model,
            system_fingerprint=response.system_fingerprint,
            choices=choices
        )

    async def handle_tgi_generate_stream(
        self,
        response: StreamResponse,
        num_generated_tokens: int,
        **kwargs
    ) -> pb2.GenerateStreamResponse:
        """cast the TGI generate stream returned object into the
           TextGenerator's GenerateStream proto.

        Args:
            response (StreamResponse): TGI response object.
            num_generated_tokens (int): number of new tokens generated by
                                        the LLM.

        Returns:
            GenerateStreamResponse: casting result.
        """
        request_id = kwargs.get("requestID")

        token = pb2.GenerateAPIToken(
            id=response.token.id,
            text=response.token.text,
            logprob=response.token.logprob,
            special=response.token.special
        )

        generated_text = response.generated_text
        # 'generated_text' is not None only in the last token
        if generated_text is not None:
            # from some mdoels the prompt might be included
            # in the 'generated_text' prompt will end with 
            # the eos_token and we can check if its present
            # in the 'generated_text' and if so - cut it
            generated_text = await self._cut_prompt_tokens(
                generated_text, request_id
            )

        details = None
        if response.details is not None:
            generated_tokens = response.details.generated_tokens
            seed = response.details.seed
            details = pb2.StreamDetails(
                finish_reason=await self._map_finish_reason(
                    response.details.finish_reason
                ),
                generated_tokens=generated_tokens,
                seed=seed
            )

        return pb2.GenerateStreamResponse(
            token=token,
            details=details,
            generated_text=generated_text
        )

    async def _is_eos_token(self, token: str) -> bool:
        return token.endswith(self._tokenizer.eos_token)

    async def _get_usage(
        self,
        messages: List[pb2.ChatCompletionMessage],
        num_generated_tokens: int
    ) -> pb2.ChatCompletionUsage:

        messages_tokens = self._tokenizer.count_tokens(
            messages=[
                MessageToDict(
                    message=message,
                    preserving_proto_field_name=True
                ) for message in messages
            ]
        )
        prompt_tokens = messages_tokens - num_generated_tokens

        return pb2.ChatCompletionUsage(
            prompt_tokens=prompt_tokens,
            completion_tokens=num_generated_tokens
        )

    async def _cast_logprobs_data_safe(
        self,
        request_id: str,
        logprobs: Dict[str, Any]
    ) -> pb2.ChatCompletionStreamLogprobsContent | None:

        if logprobs is None:
            return

        try:
            content = logprobs["content"]
            logprobs_grpc = pb2.ChatCompletionStreamLogprobsContent(
                content=[
                    pb2.ChatCompletionStreamLogprob(
                        token=logprob_data["token"],
                        logprob=logprob_data["logprob"]
                    )
                    for logprob_data in content
                ]
            )
            return logprobs_grpc

        except Exception:
            logging.warning(f"Request {request_id} failed on logprobs parsing")
            return

    async def _map_finish_reason(
        self,
        finish_reason: FinishReason
    ) -> pb2.FinishReason:

        if finish_reason == FinishReason.Length:
            return pb2.FINISH_REASON_LENGTH

        if finish_reason == FinishReason.StopSequence:
            return pb2.FINISH_REASON_STOP_SEQUENCE

        if finish_reason == FinishReason.EndOfSequenceToken:
            return pb2.FINISH_REASON_EOS_TOKEN

    async def _cut_prompt_tokens(
        self,
        generated_text: str,
        request_id: str
    ) -> str:

        for inst_token in self._inst_tokens:
            if inst_token in generated_text:
                generated_text_parts = generated_text.split(inst_token)
                return generated_text_parts[-1]

        # if we got here it means no 'inst_token' was found - we log warning
        # and return the original 'generated_text'
        logging.warning(f"Request {request_id} has no 'inst_token'")
        return generated_text

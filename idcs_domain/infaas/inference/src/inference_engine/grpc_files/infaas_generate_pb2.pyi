"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""

import builtins
import collections.abc
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import sys
import typing

if sys.version_info >= (3, 10):
    import typing as typing_extensions
else:
    import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class _FinishReason:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _FinishReasonEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_FinishReason.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    FINISH_REASON_LENGTH: _FinishReason.ValueType  # 0
    FINISH_REASON_EOS_TOKEN: _FinishReason.ValueType  # 1
    FINISH_REASON_STOP_SEQUENCE: _FinishReason.ValueType  # 2

class FinishReason(_FinishReason, metaclass=_FinishReasonEnumTypeWrapper): ...

FINISH_REASON_LENGTH: FinishReason.ValueType  # 0
FINISH_REASON_EOS_TOKEN: FinishReason.ValueType  # 1
FINISH_REASON_STOP_SEQUENCE: FinishReason.ValueType  # 2
global___FinishReason = FinishReason

@typing.final
class GenerateStreamRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    REQUESTID_FIELD_NUMBER: builtins.int
    PROMPT_FIELD_NUMBER: builtins.int
    PARAMS_FIELD_NUMBER: builtins.int
    requestID: builtins.str
    prompt: builtins.str
    @property
    def params(self) -> global___GenerateRequestParameters: ...
    def __init__(
        self,
        *,
        requestID: builtins.str = ...,
        prompt: builtins.str = ...,
        params: global___GenerateRequestParameters | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["params", b"params"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["params", b"params", "prompt", b"prompt", "requestID", b"requestID"]) -> None: ...

global___GenerateStreamRequest = GenerateStreamRequest

@typing.final
class ChatCompletionStreamRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    REQUESTID_FIELD_NUMBER: builtins.int
    MESSAGES_FIELD_NUMBER: builtins.int
    REPETITION_PENALTY_FIELD_NUMBER: builtins.int
    FREQUENCY_PENALTY_FIELD_NUMBER: builtins.int
    LOGIT_BIAS_FIELD_NUMBER: builtins.int
    RETURN_LOGPROBS_FIELD_NUMBER: builtins.int
    TOP_LOGPROBS_FIELD_NUMBER: builtins.int
    MAX_TOKENS_FIELD_NUMBER: builtins.int
    NUM_COMPLETIONS_FIELD_NUMBER: builtins.int
    PRESENCE_PENALTY_FIELD_NUMBER: builtins.int
    SEED_FIELD_NUMBER: builtins.int
    TEMPERATURE_FIELD_NUMBER: builtins.int
    TOP_P_FIELD_NUMBER: builtins.int
    requestID: builtins.str
    repetition_penalty: builtins.float
    """/ The parameter for repetition penalty. 1.0 means no penalty.
    / See [this paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.
    """
    frequency_penalty: builtins.float
    """/ The parameter for frequency penalty. 1.0 means no penalty
    / Penalize new tokens based on their existing frequency in the text so far,
    / decreasing the model's likelihood to repeat the same line verbatim.
    """
    return_logprobs: builtins.bool
    """/ Include log probabilities in the response."""
    top_logprobs: builtins.int
    """/ Include the `n` most likely tokens at each step."""
    max_tokens: builtins.int
    """/ Maximum number of generated tokens."""
    num_completions: builtins.int
    """/ Generate `n` completions."""
    presence_penalty: builtins.float
    """/ The parameter for presence penalty. 0.0 means no penalty. 
    / See [this paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.
    """
    seed: builtins.int
    """/ Random sampling seed."""
    temperature: builtins.float
    """/ The value used to module the logits distribution."""
    top_p: builtins.float
    """/ If set to < 1, only the smallest set of most probable tokens with probabilities 
    / that add up to `top_p` or higher are kept for generation
    """
    @property
    def messages(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ChatCompletionMessage]: ...
    @property
    def logit_bias(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.float]:
        """/ Adjust the likelihood of specified tokens."""

    def __init__(
        self,
        *,
        requestID: builtins.str = ...,
        messages: collections.abc.Iterable[global___ChatCompletionMessage] | None = ...,
        repetition_penalty: builtins.float | None = ...,
        frequency_penalty: builtins.float | None = ...,
        logit_bias: collections.abc.Iterable[builtins.float] | None = ...,
        return_logprobs: builtins.bool | None = ...,
        top_logprobs: builtins.int | None = ...,
        max_tokens: builtins.int | None = ...,
        num_completions: builtins.int | None = ...,
        presence_penalty: builtins.float | None = ...,
        seed: builtins.int | None = ...,
        temperature: builtins.float | None = ...,
        top_p: builtins.float | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["_frequency_penalty", b"_frequency_penalty", "_max_tokens", b"_max_tokens", "_num_completions", b"_num_completions", "_presence_penalty", b"_presence_penalty", "_repetition_penalty", b"_repetition_penalty", "_return_logprobs", b"_return_logprobs", "_seed", b"_seed", "_temperature", b"_temperature", "_top_logprobs", b"_top_logprobs", "_top_p", b"_top_p", "frequency_penalty", b"frequency_penalty", "max_tokens", b"max_tokens", "num_completions", b"num_completions", "presence_penalty", b"presence_penalty", "repetition_penalty", b"repetition_penalty", "return_logprobs", b"return_logprobs", "seed", b"seed", "temperature", b"temperature", "top_logprobs", b"top_logprobs", "top_p", b"top_p"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["_frequency_penalty", b"_frequency_penalty", "_max_tokens", b"_max_tokens", "_num_completions", b"_num_completions", "_presence_penalty", b"_presence_penalty", "_repetition_penalty", b"_repetition_penalty", "_return_logprobs", b"_return_logprobs", "_seed", b"_seed", "_temperature", b"_temperature", "_top_logprobs", b"_top_logprobs", "_top_p", b"_top_p", "frequency_penalty", b"frequency_penalty", "logit_bias", b"logit_bias", "max_tokens", b"max_tokens", "messages", b"messages", "num_completions", b"num_completions", "presence_penalty", b"presence_penalty", "repetition_penalty", b"repetition_penalty", "requestID", b"requestID", "return_logprobs", b"return_logprobs", "seed", b"seed", "temperature", b"temperature", "top_logprobs", b"top_logprobs", "top_p", b"top_p"]) -> None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_frequency_penalty", b"_frequency_penalty"]) -> typing.Literal["frequency_penalty"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_max_tokens", b"_max_tokens"]) -> typing.Literal["max_tokens"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_num_completions", b"_num_completions"]) -> typing.Literal["num_completions"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_presence_penalty", b"_presence_penalty"]) -> typing.Literal["presence_penalty"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_repetition_penalty", b"_repetition_penalty"]) -> typing.Literal["repetition_penalty"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_return_logprobs", b"_return_logprobs"]) -> typing.Literal["return_logprobs"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_seed", b"_seed"]) -> typing.Literal["seed"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_temperature", b"_temperature"]) -> typing.Literal["temperature"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_top_logprobs", b"_top_logprobs"]) -> typing.Literal["top_logprobs"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_top_p", b"_top_p"]) -> typing.Literal["top_p"] | None: ...

global___ChatCompletionStreamRequest = ChatCompletionStreamRequest

@typing.final
class ChatCompletionMessage(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    ROLE_FIELD_NUMBER: builtins.int
    CONTENT_FIELD_NUMBER: builtins.int
    role: builtins.str
    content: builtins.str
    def __init__(
        self,
        *,
        role: builtins.str = ...,
        content: builtins.str = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal["content", b"content", "role", b"role"]) -> None: ...

global___ChatCompletionMessage = ChatCompletionMessage

@typing.final
class GenerateStreamResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    TOKEN_FIELD_NUMBER: builtins.int
    TOP_TOKENS_FIELD_NUMBER: builtins.int
    GENERATED_TEXT_FIELD_NUMBER: builtins.int
    DETAILS_FIELD_NUMBER: builtins.int
    generated_text: builtins.str
    """/ Complete generated text
    / Only available when the generation is finished
    """
    @property
    def token(self) -> global___GenerateAPIToken:
        """/ Generated token"""

    @property
    def top_tokens(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___GenerateAPIToken]:
        """/ Most likely tokens"""

    @property
    def details(self) -> global___StreamDetails:
        """/ Generation details
        / Only available when the generation is finished
        """

    def __init__(
        self,
        *,
        token: global___GenerateAPIToken | None = ...,
        top_tokens: collections.abc.Iterable[global___GenerateAPIToken] | None = ...,
        generated_text: builtins.str | None = ...,
        details: global___StreamDetails | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["_details", b"_details", "_generated_text", b"_generated_text", "details", b"details", "generated_text", b"generated_text", "token", b"token"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["_details", b"_details", "_generated_text", b"_generated_text", "details", b"details", "generated_text", b"generated_text", "token", b"token", "top_tokens", b"top_tokens"]) -> None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_details", b"_details"]) -> typing.Literal["details"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_generated_text", b"_generated_text"]) -> typing.Literal["generated_text"] | None: ...

global___GenerateStreamResponse = GenerateStreamResponse

@typing.final
class ChatCompletionStreamResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    COMPLETIONID_FIELD_NUMBER: builtins.int
    OBJECT_FIELD_NUMBER: builtins.int
    CREATED_FIELD_NUMBER: builtins.int
    MODEL_FIELD_NUMBER: builtins.int
    SYSTEM_FINGERPRINT_FIELD_NUMBER: builtins.int
    CHOICES_FIELD_NUMBER: builtins.int
    completionID: builtins.str
    """/ A unique identifier for the chat completion.
    / If streaming is `true` - all chunks will have the same completion id.
    """
    object: builtins.str
    """/ The object type, which is always `chat.completion`."""
    created: builtins.int
    """/ The Unix timestamp (in seconds) of when the chat completion was created."""
    model: builtins.str
    """/ The model used for the chat completion."""
    system_fingerprint: builtins.str
    """/ This fingerprint represents the backend configuration that the model runs with."""
    @property
    def choices(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ChatCompletionStreamChoice]:
        """/ A list of chat completion choices.
        / Can be more than one if `n` is greater than 1.
        """

    def __init__(
        self,
        *,
        completionID: builtins.str = ...,
        object: builtins.str = ...,
        created: builtins.int = ...,
        model: builtins.str = ...,
        system_fingerprint: builtins.str = ...,
        choices: collections.abc.Iterable[global___ChatCompletionStreamChoice] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal["choices", b"choices", "completionID", b"completionID", "created", b"created", "model", b"model", "object", b"object", "system_fingerprint", b"system_fingerprint"]) -> None: ...

global___ChatCompletionStreamResponse = ChatCompletionStreamResponse

@typing.final
class ChatCompletionUsage(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    PROMPT_TOKENS_FIELD_NUMBER: builtins.int
    COMPLETION_TOKENS_FIELD_NUMBER: builtins.int
    prompt_tokens: builtins.int
    completion_tokens: builtins.int
    def __init__(
        self,
        *,
        prompt_tokens: builtins.int = ...,
        completion_tokens: builtins.int = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal["completion_tokens", b"completion_tokens", "prompt_tokens", b"prompt_tokens"]) -> None: ...

global___ChatCompletionUsage = ChatCompletionUsage

@typing.final
class ChatCompletionStreamChoice(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    INDEX_FIELD_NUMBER: builtins.int
    DELTA_FIELD_NUMBER: builtins.int
    LOGPROBS_FIELD_NUMBER: builtins.int
    FINISH_REASON_FIELD_NUMBER: builtins.int
    USAGE_FIELD_NUMBER: builtins.int
    index: builtins.int
    """/ Index of the chat completion"""
    finish_reason: builtins.str
    """/ Reason for completion ending"""
    @property
    def delta(self) -> global___ChatCompletionMessage:
        """/ Chunk specific data (generated token, etc.)"""

    @property
    def logprobs(self) -> global___ChatCompletionStreamLogprobsContent:
        """/ Log probabilities for the chat completion"""

    @property
    def usage(self) -> global___ChatCompletionUsage:
        """/ Usage statistics for the completion request.
        / Value is None until lst token's generation.
        """

    def __init__(
        self,
        *,
        index: builtins.int = ...,
        delta: global___ChatCompletionMessage | None = ...,
        logprobs: global___ChatCompletionStreamLogprobsContent | None = ...,
        finish_reason: builtins.str | None = ...,
        usage: global___ChatCompletionUsage | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["_delta", b"_delta", "_finish_reason", b"_finish_reason", "_logprobs", b"_logprobs", "_usage", b"_usage", "delta", b"delta", "finish_reason", b"finish_reason", "logprobs", b"logprobs", "usage", b"usage"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["_delta", b"_delta", "_finish_reason", b"_finish_reason", "_logprobs", b"_logprobs", "_usage", b"_usage", "delta", b"delta", "finish_reason", b"finish_reason", "index", b"index", "logprobs", b"logprobs", "usage", b"usage"]) -> None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_delta", b"_delta"]) -> typing.Literal["delta"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_finish_reason", b"_finish_reason"]) -> typing.Literal["finish_reason"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_logprobs", b"_logprobs"]) -> typing.Literal["logprobs"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_usage", b"_usage"]) -> typing.Literal["usage"] | None: ...

global___ChatCompletionStreamChoice = ChatCompletionStreamChoice

@typing.final
class ChatCompletionStreamLogprobsContent(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    CONTENT_FIELD_NUMBER: builtins.int
    @property
    def content(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ChatCompletionStreamLogprob]: ...
    def __init__(
        self,
        *,
        content: collections.abc.Iterable[global___ChatCompletionStreamLogprob] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal["content", b"content"]) -> None: ...

global___ChatCompletionStreamLogprobsContent = ChatCompletionStreamLogprobsContent

@typing.final
class ChatCompletionStreamLogprob(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    TOKEN_FIELD_NUMBER: builtins.int
    LOGPROB_FIELD_NUMBER: builtins.int
    token: builtins.str
    logprob: builtins.float
    def __init__(
        self,
        *,
        token: builtins.str = ...,
        logprob: builtins.float = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal["logprob", b"logprob", "token", b"token"]) -> None: ...

global___ChatCompletionStreamLogprob = ChatCompletionStreamLogprob

@typing.final
class GenerateAPIToken(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    ID_FIELD_NUMBER: builtins.int
    TEXT_FIELD_NUMBER: builtins.int
    LOGPROB_FIELD_NUMBER: builtins.int
    SPECIAL_FIELD_NUMBER: builtins.int
    id: builtins.int
    """/ Token ID from the model tokenizer"""
    text: builtins.str
    """/ Token text"""
    logprob: builtins.float
    """/ Logprob"""
    special: builtins.bool
    """/ Is the token a special token
    / Can be used to ignore tokens when concatenating
    """
    def __init__(
        self,
        *,
        id: builtins.int = ...,
        text: builtins.str = ...,
        logprob: builtins.float | None = ...,
        special: builtins.bool = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["_logprob", b"_logprob", "logprob", b"logprob"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["_logprob", b"_logprob", "id", b"id", "logprob", b"logprob", "special", b"special", "text", b"text"]) -> None: ...
    def WhichOneof(self, oneof_group: typing.Literal["_logprob", b"_logprob"]) -> typing.Literal["logprob"] | None: ...

global___GenerateAPIToken = GenerateAPIToken

@typing.final
class StreamDetails(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    FINISH_REASON_FIELD_NUMBER: builtins.int
    GENERATED_TOKENS_FIELD_NUMBER: builtins.int
    SEED_FIELD_NUMBER: builtins.int
    finish_reason: global___FinishReason.ValueType
    """/ Generation finish reason"""
    generated_tokens: builtins.int
    """/ Number of generated tokens"""
    seed: builtins.int
    """/ Sampling seed if sampling was activated"""
    def __init__(
        self,
        *,
        finish_reason: global___FinishReason.ValueType = ...,
        generated_tokens: builtins.int = ...,
        seed: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["_seed", b"_seed", "seed", b"seed"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["_seed", b"_seed", "finish_reason", b"finish_reason", "generated_tokens", b"generated_tokens", "seed", b"seed"]) -> None: ...
    def WhichOneof(self, oneof_group: typing.Literal["_seed", b"_seed"]) -> typing.Literal["seed"] | None: ...

global___StreamDetails = StreamDetails

@typing.final
class GenerateRequestParameters(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DO_SAMPLE_FIELD_NUMBER: builtins.int
    MAX_NEW_TOKENS_FIELD_NUMBER: builtins.int
    REPETITION_PENALTY_FIELD_NUMBER: builtins.int
    FREQUENCY_PENALTY_FIELD_NUMBER: builtins.int
    RETURN_FULL_TEXT_FIELD_NUMBER: builtins.int
    STOP_FIELD_NUMBER: builtins.int
    SEED_FIELD_NUMBER: builtins.int
    TEMPERATURE_FIELD_NUMBER: builtins.int
    TOP_K_FIELD_NUMBER: builtins.int
    TOP_P_FIELD_NUMBER: builtins.int
    TRUNCATE_FIELD_NUMBER: builtins.int
    TYPICAL_P_FIELD_NUMBER: builtins.int
    BEST_OF_FIELD_NUMBER: builtins.int
    WATERMARK_FIELD_NUMBER: builtins.int
    DETAILS_FIELD_NUMBER: builtins.int
    DECODER_INPUT_DETAILS_FIELD_NUMBER: builtins.int
    TOP_N_TOKENS_FIELD_NUMBER: builtins.int
    do_sample: builtins.bool
    """/ Activate logits sampling"""
    max_new_tokens: builtins.int
    """/ Maximum number of generated tokens"""
    repetition_penalty: builtins.float
    """/ The parameter for repetition penalty. 1.0 means no penalty.
    / See [this paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.
    """
    frequency_penalty: builtins.float
    """/ The parameter for frequency penalty. 1.0 means no penalty
    / Penalize new tokens based on their existing frequency in the text so far,
    / decreasing the model's likelihood to repeat the same line verbatim.
    """
    return_full_text: builtins.bool
    """/ Whether to prepend the prompt to the generated text"""
    seed: builtins.int
    """/ Random sampling seed"""
    temperature: builtins.float
    """/ The value used to module the logits distribution."""
    top_k: builtins.int
    """/ The number of highest probability vocabulary tokens to keep for top-k-filtering."""
    top_p: builtins.float
    """/ If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or
    / higher are kept for generation.
    """
    truncate: builtins.int
    """/ truncate inputs tokens to the given size"""
    typical_p: builtins.float
    """/ Typical Decoding mass
    / See [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666) for more information
    """
    best_of: builtins.int
    """/ Generate best_of sequences and return the one if the highest token logprobs"""
    watermark: builtins.bool
    """/ Watermarking with [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)"""
    details: builtins.bool
    """/ Get generation details"""
    decoder_input_details: builtins.bool
    """/ Get decoder input token logprobs and ids"""
    top_n_tokens: builtins.int
    """/ Return the N most likely tokens at each step
    / grammars are not supported at the moment
    """
    @property
    def stop(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.str]:
        """/ Stop generating tokens if a member of `stop_sequences` is generated"""

    def __init__(
        self,
        *,
        do_sample: builtins.bool = ...,
        max_new_tokens: builtins.int = ...,
        repetition_penalty: builtins.float | None = ...,
        frequency_penalty: builtins.float | None = ...,
        return_full_text: builtins.bool = ...,
        stop: collections.abc.Iterable[builtins.str] | None = ...,
        seed: builtins.int | None = ...,
        temperature: builtins.float | None = ...,
        top_k: builtins.int | None = ...,
        top_p: builtins.float | None = ...,
        truncate: builtins.int | None = ...,
        typical_p: builtins.float | None = ...,
        best_of: builtins.int | None = ...,
        watermark: builtins.bool = ...,
        details: builtins.bool = ...,
        decoder_input_details: builtins.bool = ...,
        top_n_tokens: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["_best_of", b"_best_of", "_frequency_penalty", b"_frequency_penalty", "_repetition_penalty", b"_repetition_penalty", "_seed", b"_seed", "_temperature", b"_temperature", "_top_k", b"_top_k", "_top_n_tokens", b"_top_n_tokens", "_top_p", b"_top_p", "_truncate", b"_truncate", "_typical_p", b"_typical_p", "best_of", b"best_of", "frequency_penalty", b"frequency_penalty", "repetition_penalty", b"repetition_penalty", "seed", b"seed", "temperature", b"temperature", "top_k", b"top_k", "top_n_tokens", b"top_n_tokens", "top_p", b"top_p", "truncate", b"truncate", "typical_p", b"typical_p"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["_best_of", b"_best_of", "_frequency_penalty", b"_frequency_penalty", "_repetition_penalty", b"_repetition_penalty", "_seed", b"_seed", "_temperature", b"_temperature", "_top_k", b"_top_k", "_top_n_tokens", b"_top_n_tokens", "_top_p", b"_top_p", "_truncate", b"_truncate", "_typical_p", b"_typical_p", "best_of", b"best_of", "decoder_input_details", b"decoder_input_details", "details", b"details", "do_sample", b"do_sample", "frequency_penalty", b"frequency_penalty", "max_new_tokens", b"max_new_tokens", "repetition_penalty", b"repetition_penalty", "return_full_text", b"return_full_text", "seed", b"seed", "stop", b"stop", "temperature", b"temperature", "top_k", b"top_k", "top_n_tokens", b"top_n_tokens", "top_p", b"top_p", "truncate", b"truncate", "typical_p", b"typical_p", "watermark", b"watermark"]) -> None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_best_of", b"_best_of"]) -> typing.Literal["best_of"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_frequency_penalty", b"_frequency_penalty"]) -> typing.Literal["frequency_penalty"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_repetition_penalty", b"_repetition_penalty"]) -> typing.Literal["repetition_penalty"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_seed", b"_seed"]) -> typing.Literal["seed"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_temperature", b"_temperature"]) -> typing.Literal["temperature"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_top_k", b"_top_k"]) -> typing.Literal["top_k"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_top_n_tokens", b"_top_n_tokens"]) -> typing.Literal["top_n_tokens"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_top_p", b"_top_p"]) -> typing.Literal["top_p"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_truncate", b"_truncate"]) -> typing.Literal["truncate"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_typical_p", b"_typical_p"]) -> typing.Literal["typical_p"] | None: ...

global___GenerateRequestParameters = GenerateRequestParameters

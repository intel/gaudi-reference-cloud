apiVersion: v1
kind: Pod
metadata:
  name: tgi-pod
  labels:
    app: tgi-server
spec:
  runtimeClassName: habana
  
  initContainers:
    - name: s3-model-mount
      image: amazon/aws-cli:latest
      command: ["/bin/sh", "-c"]
      args:
        - |
          aws s3 sync s3://cnvrg-test-maas/models/${MODEL_ID} /model &&
          chmod -R 400 /model
      env:
        - name: MODEL_ID
          value: mistralai/Mistral-7B-Instruct-v0.1
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: aws-access-secrets
              key: AWS_ACCESS_KEY_ID 
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: aws-access-secrets
              key: AWS_SECRET_ACCESS_KEY
      volumeMounts:
        - name: models
          mountPath: /model

  containers:
  - name: tgi-mistral-0-1-7b
    image: moditamam/tgi-gaudi:2.0.6
    securityContext:
      capabilities:
        add: ["SYS_PTRACE"]
    volumeMounts:  
      - name: models
        mountPath: /usr/src/model
        readOnly: true
    env:
      - name: HABANA_VISIBLE_DEVICES
        value: "all"
      - name: OMPI_MCA_btl_vader_single_copy_mechanism
        value: "none"
      - name: TEXT_GENERATION_SERVER_IGNORE_EOS_TOKEN
        value: "false"
      - name: PT_HPU_ENABLE_LAZY_COLLECTIVES
        value: "true"
      - name: BATCH_BUCKET_SIZE
        value: "32"
      - name: PREFILL_BATCH_BUCKET_SIZE
        value: "4"
      - name: PAD_SEQUENCE_TO_MULTIPLE_OF
        value: "64"
      - name: LIMIT_HPU_GRAPH
        value: "true"
      - name: USE_FLASH_ATTENTION
        value: "true"
      - name: FLASH_ATTENTION_RECOMPUTE
        value: "true"
      - name: MODEL_ID
        value: /usr/src/model
      - name: SHARDED
        value: "false"
      # - name: NUM_SHARD
      #   value: "8"
      - name: WARMUP_ENABLED
        value: "true"
    resources:
      limits:
        habana.ai/gaudi: "1"
    args:
      - --max-input-length
      - "2048"
      - --max-batch-prefill-tokens
      - "4096"
      - --max-total-tokens
      - "4096"
      - --max-batch-total-tokens
      - "65536"
      - --max-waiting-tokens
      - "7"
      - --waiting-served-ratio
      - "1.2"
      - --max-concurrent-requests
      - "64"
      - --hostname
      - "0.0.0.0"
      - --port
      - "8080"

  volumes:
  - name: models
    emptyDir: {}
  - name: fp8-config-volume
    emptyDir: {}

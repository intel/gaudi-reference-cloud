apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "infaas-safeguard.fullname" . }}
  labels:
    {{- include "infaas-safeguard.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      app: {{ include "infaas-safeguard.fullname" . }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        app: {{ include "infaas-safeguard.fullname" . }}
    spec:
      runtimeClassName: habana
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      containers:
        - name: tgi-safeguard
          securityContext:
            capabilities:
              add: ["SYS_NICE"]
          image: {{ .Values.image.tgiSafeguard.repository }}:{{ .Values.image.tgiSafeguard.tag }}
          imagePullPolicy: {{ .Values.image.tgiSafeguard.pullPolicy }}
          args:
            - --max-input-length
            - "2048"
            - --max-batch-prefill-tokens
            - "4096"
            - --max-total-tokens
            - "4096"
            - --max-batch-total-tokens
            - "131072"
            - --max-waiting-tokens
            - "5"
            - --waiting-served-ratio
            - "1.1"
            - --max-concurrent-requests
            - "128"
            - "--hostname"
            - "0.0.0.0"
            - "--port"
            - "9000"
          env:
            - name: HABANA_VISIBLE_DEVICES
              value: "all"
            - name: OMPI_MCA_btl_vader_single_copy_mechanism
              value: "none"
            - name: TEXT_GENERATION_SERVER_IGNORE_EOS_TOKEN
              value: "false"
            - name: PT_HPU_ENABLE_LAZY_COLLECTIVES
              value: "true"
            - name: BATCH_BUCKET_SIZE
              value: "64"
            - name: PREFILL_BATCH_BUCKET_SIZE
              value: "2"
            - name: PAD_SEQUENCE_TO_MULTIPLE_OF
              value: "64"
            - name: USE_FLASH_ATTENTION
              value: "true"
            - name: FLASH_ATTENTION_RECOMPUTE
              value: "true"
            - name: MODEL_ID
              value: meta-llama/Llama-Guard-3-8B
            - name: HUGGINGFACE_HUB_CACHE
              value: /models-cache
            - name: TGI_PROFILER_ENABLED
              value: "true"
            - name: SHARDED
              value: "false"
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-api-token-secret
                  key: HF_API_TOKEN
          startupProbe:
            httpGet:
              port: 9000
              path: /info
            initialDelaySeconds: 50
            periodSeconds: 5
            timeoutSeconds: 2
            failureThreshold: 48
          livenessProbe:
            httpGet:
              port: 9000
              path: /info
            periodSeconds: 5
            timeoutSeconds: 2
            failureThreshold: 12
          readinessProbe:
            exec:
              command:
                - /bin/bash
                - -c
                - | 
                  URL="http://localhost:9000/generate"
                  DATA='{"inputs":"hi", "parameters":{"max_new_tokens":2}}'
                  curl --fail -X POST "$URL" -d "$DATA" -H "Content-Type: application/json"
            periodSeconds: 10
            timeoutSeconds: 3
            failureThreshold: 6
          resources:
            limits:
              habana.ai/gaudi: "1"
          volumeMounts:
            - mountPath: models-cache
              name: models-cache

      {{- with .Values.volumes }}
      volumes:
        {{- toYaml . | nindent 8 }}
      {{- end }}